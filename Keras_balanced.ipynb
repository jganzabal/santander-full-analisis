{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.layers import Dense, Activation, BatchNormalization, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import adam\n",
    "from sklearn.metrics import roc_auc_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_SET = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_153705</td>\n",
       "      <td>0</td>\n",
       "      <td>12.3251</td>\n",
       "      <td>3.7835</td>\n",
       "      <td>11.6726</td>\n",
       "      <td>4.4771</td>\n",
       "      <td>11.1765</td>\n",
       "      <td>-5.6920</td>\n",
       "      <td>5.8932</td>\n",
       "      <td>12.4260</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.2082</td>\n",
       "      <td>4.2236</td>\n",
       "      <td>4.3090</td>\n",
       "      <td>3.4810</td>\n",
       "      <td>14.9629</td>\n",
       "      <td>-0.7598</td>\n",
       "      <td>7.9183</td>\n",
       "      <td>8.9676</td>\n",
       "      <td>11.4002</td>\n",
       "      <td>-4.7609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_178870</td>\n",
       "      <td>0</td>\n",
       "      <td>11.8970</td>\n",
       "      <td>5.0792</td>\n",
       "      <td>7.9301</td>\n",
       "      <td>6.4788</td>\n",
       "      <td>9.3639</td>\n",
       "      <td>4.2042</td>\n",
       "      <td>6.5899</td>\n",
       "      <td>16.3848</td>\n",
       "      <td>...</td>\n",
       "      <td>3.8722</td>\n",
       "      <td>11.2516</td>\n",
       "      <td>0.2554</td>\n",
       "      <td>6.9514</td>\n",
       "      <td>15.9305</td>\n",
       "      <td>-2.5133</td>\n",
       "      <td>-0.6977</td>\n",
       "      <td>9.6821</td>\n",
       "      <td>19.7626</td>\n",
       "      <td>10.4399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_77100</td>\n",
       "      <td>0</td>\n",
       "      <td>8.1522</td>\n",
       "      <td>-6.1382</td>\n",
       "      <td>14.0154</td>\n",
       "      <td>11.3520</td>\n",
       "      <td>12.0944</td>\n",
       "      <td>-3.0461</td>\n",
       "      <td>4.0223</td>\n",
       "      <td>20.1020</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0441</td>\n",
       "      <td>8.9315</td>\n",
       "      <td>2.0132</td>\n",
       "      <td>4.9495</td>\n",
       "      <td>21.1372</td>\n",
       "      <td>1.0498</td>\n",
       "      <td>12.0516</td>\n",
       "      <td>8.4104</td>\n",
       "      <td>9.4328</td>\n",
       "      <td>14.7863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_172656</td>\n",
       "      <td>1</td>\n",
       "      <td>13.2053</td>\n",
       "      <td>-2.9593</td>\n",
       "      <td>14.1576</td>\n",
       "      <td>7.0573</td>\n",
       "      <td>13.3132</td>\n",
       "      <td>-15.6966</td>\n",
       "      <td>4.0718</td>\n",
       "      <td>16.2601</td>\n",
       "      <td>...</td>\n",
       "      <td>10.1823</td>\n",
       "      <td>6.6488</td>\n",
       "      <td>3.2586</td>\n",
       "      <td>6.2694</td>\n",
       "      <td>17.6702</td>\n",
       "      <td>0.4203</td>\n",
       "      <td>11.0431</td>\n",
       "      <td>9.5062</td>\n",
       "      <td>17.0413</td>\n",
       "      <td>6.4602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_130313</td>\n",
       "      <td>1</td>\n",
       "      <td>13.9468</td>\n",
       "      <td>4.3059</td>\n",
       "      <td>12.0891</td>\n",
       "      <td>7.8422</td>\n",
       "      <td>12.7303</td>\n",
       "      <td>-19.9832</td>\n",
       "      <td>6.2974</td>\n",
       "      <td>15.0854</td>\n",
       "      <td>...</td>\n",
       "      <td>3.1895</td>\n",
       "      <td>4.7566</td>\n",
       "      <td>2.3023</td>\n",
       "      <td>8.0319</td>\n",
       "      <td>18.6647</td>\n",
       "      <td>1.5099</td>\n",
       "      <td>2.3106</td>\n",
       "      <td>8.2752</td>\n",
       "      <td>14.2200</td>\n",
       "      <td>4.9702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID_code  target    var_0   var_1    var_2    var_3    var_4    var_5  \\\n",
       "0  train_153705       0  12.3251  3.7835  11.6726   4.4771  11.1765  -5.6920   \n",
       "1  train_178870       0  11.8970  5.0792   7.9301   6.4788   9.3639   4.2042   \n",
       "2   train_77100       0   8.1522 -6.1382  14.0154  11.3520  12.0944  -3.0461   \n",
       "3  train_172656       1  13.2053 -2.9593  14.1576   7.0573  13.3132 -15.6966   \n",
       "4  train_130313       1  13.9468  4.3059  12.0891   7.8422  12.7303 -19.9832   \n",
       "\n",
       "    var_6    var_7   ...     var_190  var_191  var_192  var_193  var_194  \\\n",
       "0  5.8932  12.4260   ...     -2.2082   4.2236   4.3090   3.4810  14.9629   \n",
       "1  6.5899  16.3848   ...      3.8722  11.2516   0.2554   6.9514  15.9305   \n",
       "2  4.0223  20.1020   ...     -0.0441   8.9315   2.0132   4.9495  21.1372   \n",
       "3  4.0718  16.2601   ...     10.1823   6.6488   3.2586   6.2694  17.6702   \n",
       "4  6.2974  15.0854   ...      3.1895   4.7566   2.3023   8.0319  18.6647   \n",
       "\n",
       "   var_195  var_196  var_197  var_198  var_199  \n",
       "0  -0.7598   7.9183   8.9676  11.4002  -4.7609  \n",
       "1  -2.5133  -0.6977   9.6821  19.7626  10.4399  \n",
       "2   1.0498  12.0516   8.4104   9.4328  14.7863  \n",
       "3   0.4203  11.0431   9.5062  17.0413   6.4602  \n",
       "4   1.5099   2.3106   8.2752  14.2200   4.9702  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f'data/working_train_df_{CURRENT_SET}.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39308</th>\n",
       "      <td>train_39308</td>\n",
       "      <td>0</td>\n",
       "      <td>6.2235</td>\n",
       "      <td>-3.6087</td>\n",
       "      <td>11.7680</td>\n",
       "      <td>6.0052</td>\n",
       "      <td>11.5614</td>\n",
       "      <td>-7.4537</td>\n",
       "      <td>4.7117</td>\n",
       "      <td>20.0163</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0174</td>\n",
       "      <td>6.4734</td>\n",
       "      <td>2.3937</td>\n",
       "      <td>1.4221</td>\n",
       "      <td>20.6887</td>\n",
       "      <td>2.9036</td>\n",
       "      <td>10.3056</td>\n",
       "      <td>9.0615</td>\n",
       "      <td>18.2392</td>\n",
       "      <td>-2.6482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179236</th>\n",
       "      <td>train_179236</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0275</td>\n",
       "      <td>-6.8930</td>\n",
       "      <td>10.1656</td>\n",
       "      <td>6.1174</td>\n",
       "      <td>9.5721</td>\n",
       "      <td>-7.2208</td>\n",
       "      <td>6.8899</td>\n",
       "      <td>12.1197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5192</td>\n",
       "      <td>6.7129</td>\n",
       "      <td>1.3789</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>15.7231</td>\n",
       "      <td>0.9399</td>\n",
       "      <td>2.1101</td>\n",
       "      <td>10.5431</td>\n",
       "      <td>15.5558</td>\n",
       "      <td>4.3572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113362</th>\n",
       "      <td>train_113362</td>\n",
       "      <td>0</td>\n",
       "      <td>12.3167</td>\n",
       "      <td>-0.2938</td>\n",
       "      <td>9.0521</td>\n",
       "      <td>6.3380</td>\n",
       "      <td>13.6952</td>\n",
       "      <td>-4.3922</td>\n",
       "      <td>5.6375</td>\n",
       "      <td>16.5737</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0286</td>\n",
       "      <td>1.6299</td>\n",
       "      <td>2.5299</td>\n",
       "      <td>8.5810</td>\n",
       "      <td>16.9100</td>\n",
       "      <td>0.5776</td>\n",
       "      <td>4.0023</td>\n",
       "      <td>9.9524</td>\n",
       "      <td>20.8813</td>\n",
       "      <td>-24.0936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94934</th>\n",
       "      <td>train_94934</td>\n",
       "      <td>0</td>\n",
       "      <td>8.5020</td>\n",
       "      <td>-4.5557</td>\n",
       "      <td>12.3567</td>\n",
       "      <td>6.0692</td>\n",
       "      <td>12.4196</td>\n",
       "      <td>-9.6285</td>\n",
       "      <td>4.5737</td>\n",
       "      <td>18.6622</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.7295</td>\n",
       "      <td>3.6510</td>\n",
       "      <td>3.0502</td>\n",
       "      <td>3.5242</td>\n",
       "      <td>16.1205</td>\n",
       "      <td>-1.8683</td>\n",
       "      <td>8.2394</td>\n",
       "      <td>8.4591</td>\n",
       "      <td>18.9237</td>\n",
       "      <td>-24.0737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20509</th>\n",
       "      <td>train_20509</td>\n",
       "      <td>1</td>\n",
       "      <td>15.1258</td>\n",
       "      <td>-7.7021</td>\n",
       "      <td>16.3087</td>\n",
       "      <td>8.5685</td>\n",
       "      <td>11.8125</td>\n",
       "      <td>0.3309</td>\n",
       "      <td>4.9676</td>\n",
       "      <td>16.6815</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.4755</td>\n",
       "      <td>15.7240</td>\n",
       "      <td>-1.2651</td>\n",
       "      <td>7.6492</td>\n",
       "      <td>21.7630</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>6.9437</td>\n",
       "      <td>10.1212</td>\n",
       "      <td>16.8096</td>\n",
       "      <td>5.2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID_code  target    var_0   var_1    var_2   var_3    var_4  \\\n",
       "39308    train_39308       0   6.2235 -3.6087  11.7680  6.0052  11.5614   \n",
       "179236  train_179236       0  10.0275 -6.8930  10.1656  6.1174   9.5721   \n",
       "113362  train_113362       0  12.3167 -0.2938   9.0521  6.3380  13.6952   \n",
       "94934    train_94934       0   8.5020 -4.5557  12.3567  6.0692  12.4196   \n",
       "20509    train_20509       1  15.1258 -7.7021  16.3087  8.5685  11.8125   \n",
       "\n",
       "         var_5   var_6    var_7   ...     var_190  var_191  var_192  var_193  \\\n",
       "39308  -7.4537  4.7117  20.0163   ...     -0.0174   6.4734   2.3937   1.4221   \n",
       "179236 -7.2208  6.8899  12.1197   ...      0.5192   6.7129   1.3789   0.1898   \n",
       "113362 -4.3922  5.6375  16.5737   ...      9.0286   1.6299   2.5299   8.5810   \n",
       "94934  -9.6285  4.5737  18.6622   ...     -2.7295   3.6510   3.0502   3.5242   \n",
       "20509   0.3309  4.9676  16.6815   ...     -2.4755  15.7240  -1.2651   7.6492   \n",
       "\n",
       "        var_194  var_195  var_196  var_197  var_198  var_199  \n",
       "39308   20.6887   2.9036  10.3056   9.0615  18.2392  -2.6482  \n",
       "179236  15.7231   0.9399   2.1101  10.5431  15.5558   4.3572  \n",
       "113362  16.9100   0.5776   4.0023   9.9524  20.8813 -24.0936  \n",
       "94934   16.1205  -1.8683   8.2394   8.4591  18.9237 -24.0737  \n",
       "20509   21.7630   0.2565   6.9437  10.1212  16.8096   5.2019  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid = pd.read_csv(f'data/valid_splitted.csv')\n",
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.drop(columns=['ID_code', 'target']).values\n",
    "y_train = df['target'].values\n",
    "\n",
    "X_valid = df_valid.drop(columns=['ID_code', 'target']).values\n",
    "y_valid = df_valid['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34046, 200) (34046,)\n",
      "(40000, 200) (40000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(x,y,t=2):\n",
    "    xs,xn = [],[]\n",
    "    for i in range(t):\n",
    "        x1 = x[y==1].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        for c in range(x1.shape[1]):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:,c] = x1[:,c][ids]\n",
    "        xs.append(x1)\n",
    "\n",
    "    for i in range(t//2):\n",
    "        x1 = x[y==0].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        for c in range(x1.shape[1]):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:,c] = x1[:,c][ids]\n",
    "        xn.append(x1)\n",
    "\n",
    "    xs = np.vstack(xs)\n",
    "    xn = np.vstack(xn)\n",
    "    ys = np.ones(xs.shape[0])\n",
    "    yn = np.zeros(xn.shape[0])\n",
    "    x = np.vstack([x,xs,xn])\n",
    "    y = np.concatenate([y,ys,yn])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.1 s, sys: 108 ms, total: 1.21 s\n",
      "Wall time: 231 ms\n"
     ]
    }
   ],
   "source": [
    "# CPU times: user 13.3 s, sys: 8.3 s, total: 21.6 s\n",
    "# Wall time: 18 s\n",
    "# CPU times: user 3.1 s, sys: 405 ms, total: 3.51 s\n",
    "# Wall time: 1.03 s\n",
    "%time X_train_aug, y_train_aug = augment(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug_sqr = X_train_aug\n",
    "X_valid_sqr = X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_aug_sqr = np.append(X_train_aug, X_train_aug**2, axis=1)\n",
    "# X_train_aug_sqr = np.append(X_train_aug_sqr, X_train_aug**3, axis=1)\n",
    "# X_valid_sqr = np.append(X_valid, X_valid**2, axis=1)\n",
    "# X_valid_sqr = np.append(X_valid_sqr, X_valid**3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80250, 200), (8018, 200))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_aug_sqr.shape, X_valid_sqr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_aug_cat = np.array([y_train_aug, 1-y_train_aug]).T\n",
    "y_valid_cat = np.array([y_valid, 1-y_valid]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model solo con train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l1_reg = l1(l=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 82,402\n",
      "Trainable params: 81,602\n",
      "Non-trainable params: 800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "p_do = 0.5\n",
    "model.add(Dense(200, input_shape=(X_train_aug_sqr.shape[1],))) #kernel_regularizer=l1_reg,\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(p_do))\n",
    "\n",
    "model.add(Dense(200))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(p_do))\n",
    "\n",
    "# model.add(Dense(200))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(p_do))\n",
    "\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from clr import LRFinder\n",
    "\n",
    "# model.compile('SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# batch_size = 256\n",
    "# lr_callback = LRFinder(len(X_train_aug), batch_size,\n",
    "#                        1e-6, 1,\n",
    "#                        # validation_data=(X_val, Y_val),\n",
    "#                        lr_scale='exp', save_dir='data')\n",
    "\n",
    "# # Ensure that number of epochs = 1 when calling fit()\n",
    "# model.fit(X_train_aug, y_train_aug_cat, epochs=1, batch_size=batch_size, callbacks=[lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_callback.plot_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from clr import OneCycleLR\n",
    "\n",
    "# lr_manager = OneCycleLR(1e-2)\n",
    "                        \n",
    "# model.fit(X_train_aug, y_train_aug_cat, \n",
    "#           epochs=10, \n",
    "#           batch_size=batch_size, \n",
    "#           validation_data=(X_test, y_test_cat), \n",
    "#           callbacks=[lr_manager])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('best.hdf5', monitor='val_loss', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "optimizer = adam(lr=1e-4)\n",
    "model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80250 samples, validate on 8018 samples\n",
      "Epoch 1/75\n",
      "80250/80250 [==============================] - 2s 21us/step - loss: 0.8289 - acc: 0.5645 - val_loss: 0.6125 - val_acc: 0.6621\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61254, saving model to best.hdf5\n",
      "Epoch 2/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.7011 - acc: 0.6249 - val_loss: 0.5715 - val_acc: 0.7093\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.61254 to 0.57148, saving model to best.hdf5\n",
      "Epoch 3/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.6425 - acc: 0.6596 - val_loss: 0.5482 - val_acc: 0.7284\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.57148 to 0.54824, saving model to best.hdf5\n",
      "Epoch 4/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.6007 - acc: 0.6847 - val_loss: 0.5351 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.54824 to 0.53505, saving model to best.hdf5\n",
      "Epoch 5/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.5721 - acc: 0.7056 - val_loss: 0.5253 - val_acc: 0.7461\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.53505 to 0.52527, saving model to best.hdf5\n",
      "Epoch 6/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.5526 - acc: 0.7186 - val_loss: 0.5189 - val_acc: 0.7482\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.52527 to 0.51887, saving model to best.hdf5\n",
      "Epoch 7/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.5367 - acc: 0.7304 - val_loss: 0.5136 - val_acc: 0.7517\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.51887 to 0.51360, saving model to best.hdf5\n",
      "Epoch 8/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.5265 - acc: 0.7368 - val_loss: 0.5095 - val_acc: 0.7543\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.51360 to 0.50954, saving model to best.hdf5\n",
      "Epoch 9/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.5185 - acc: 0.7431 - val_loss: 0.5060 - val_acc: 0.7562\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.50954 to 0.50602, saving model to best.hdf5\n",
      "Epoch 10/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.5111 - acc: 0.7492 - val_loss: 0.5037 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.50602 to 0.50367, saving model to best.hdf5\n",
      "Epoch 11/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.5035 - acc: 0.7534 - val_loss: 0.4996 - val_acc: 0.7600\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.50367 to 0.49962, saving model to best.hdf5\n",
      "Epoch 12/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.5015 - acc: 0.7532 - val_loss: 0.4980 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.49962 to 0.49805, saving model to best.hdf5\n",
      "Epoch 13/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4953 - acc: 0.7588 - val_loss: 0.4950 - val_acc: 0.7640\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.49805 to 0.49500, saving model to best.hdf5\n",
      "Epoch 14/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4908 - acc: 0.7620 - val_loss: 0.4931 - val_acc: 0.7650\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.49500 to 0.49314, saving model to best.hdf5\n",
      "Epoch 15/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4878 - acc: 0.7636 - val_loss: 0.4912 - val_acc: 0.7660\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.49314 to 0.49118, saving model to best.hdf5\n",
      "Epoch 16/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4856 - acc: 0.7669 - val_loss: 0.4894 - val_acc: 0.7663\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.49118 to 0.48938, saving model to best.hdf5\n",
      "Epoch 17/75\n",
      "80250/80250 [==============================] - 1s 12us/step - loss: 0.4825 - acc: 0.7666 - val_loss: 0.4884 - val_acc: 0.7674\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.48938 to 0.48838, saving model to best.hdf5\n",
      "Epoch 18/75\n",
      "80250/80250 [==============================] - 1s 12us/step - loss: 0.4800 - acc: 0.7688 - val_loss: 0.4862 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.48838 to 0.48622, saving model to best.hdf5\n",
      "Epoch 19/75\n",
      "80250/80250 [==============================] - 1s 12us/step - loss: 0.4779 - acc: 0.7704 - val_loss: 0.4852 - val_acc: 0.7685\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.48622 to 0.48517, saving model to best.hdf5\n",
      "Epoch 20/75\n",
      "80250/80250 [==============================] - 1s 12us/step - loss: 0.4775 - acc: 0.7718 - val_loss: 0.4845 - val_acc: 0.7679\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.48517 to 0.48454, saving model to best.hdf5\n",
      "Epoch 21/75\n",
      "80250/80250 [==============================] - 1s 12us/step - loss: 0.4739 - acc: 0.7732 - val_loss: 0.4839 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.48454 to 0.48391, saving model to best.hdf5\n",
      "Epoch 22/75\n",
      "80250/80250 [==============================] - 1s 12us/step - loss: 0.4709 - acc: 0.7757 - val_loss: 0.4826 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.48391 to 0.48257, saving model to best.hdf5\n",
      "Epoch 23/75\n",
      "80250/80250 [==============================] - 1s 12us/step - loss: 0.4683 - acc: 0.7774 - val_loss: 0.4814 - val_acc: 0.7693\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.48257 to 0.48141, saving model to best.hdf5\n",
      "Epoch 24/75\n",
      "80250/80250 [==============================] - 1s 12us/step - loss: 0.4696 - acc: 0.7759 - val_loss: 0.4805 - val_acc: 0.7716\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.48141 to 0.48052, saving model to best.hdf5\n",
      "Epoch 25/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4661 - acc: 0.7782 - val_loss: 0.4791 - val_acc: 0.7716\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.48052 to 0.47906, saving model to best.hdf5\n",
      "Epoch 26/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4648 - acc: 0.7793 - val_loss: 0.4788 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.47906 to 0.47876, saving model to best.hdf5\n",
      "Epoch 27/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4646 - acc: 0.7796 - val_loss: 0.4781 - val_acc: 0.7726\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.47876 to 0.47809, saving model to best.hdf5\n",
      "Epoch 28/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4604 - acc: 0.7817 - val_loss: 0.4771 - val_acc: 0.7730\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.47809 to 0.47715, saving model to best.hdf5\n",
      "Epoch 29/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4613 - acc: 0.7809 - val_loss: 0.4766 - val_acc: 0.7716\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.47715 to 0.47663, saving model to best.hdf5\n",
      "Epoch 30/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4598 - acc: 0.7835 - val_loss: 0.4758 - val_acc: 0.7735\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.47663 to 0.47582, saving model to best.hdf5\n",
      "Epoch 31/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4588 - acc: 0.7822 - val_loss: 0.4753 - val_acc: 0.7730\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.47582 to 0.47530, saving model to best.hdf5\n",
      "Epoch 32/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4561 - acc: 0.7843 - val_loss: 0.4758 - val_acc: 0.7721\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.47530\n",
      "Epoch 33/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4555 - acc: 0.7859 - val_loss: 0.4746 - val_acc: 0.7721\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.47530 to 0.47463, saving model to best.hdf5\n",
      "Epoch 34/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4550 - acc: 0.7874 - val_loss: 0.4741 - val_acc: 0.7725\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.47463 to 0.47413, saving model to best.hdf5\n",
      "Epoch 35/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4540 - acc: 0.7864 - val_loss: 0.4735 - val_acc: 0.7726\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.47413 to 0.47349, saving model to best.hdf5\n",
      "Epoch 36/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4527 - acc: 0.7867 - val_loss: 0.4726 - val_acc: 0.7745\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.47349 to 0.47263, saving model to best.hdf5\n",
      "Epoch 37/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4506 - acc: 0.7876 - val_loss: 0.4726 - val_acc: 0.7741\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.47263 to 0.47260, saving model to best.hdf5\n",
      "Epoch 38/75\n",
      "80250/80250 [==============================] - 1s 12us/step - loss: 0.4495 - acc: 0.7895 - val_loss: 0.4715 - val_acc: 0.7743\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.47260 to 0.47147, saving model to best.hdf5\n",
      "Epoch 39/75\n",
      "80250/80250 [==============================] - 1s 12us/step - loss: 0.4494 - acc: 0.7895 - val_loss: 0.4714 - val_acc: 0.7749\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.47147 to 0.47137, saving model to best.hdf5\n",
      "Epoch 40/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4488 - acc: 0.7901 - val_loss: 0.4706 - val_acc: 0.7756\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.47137 to 0.47062, saving model to best.hdf5\n",
      "Epoch 41/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4491 - acc: 0.7906 - val_loss: 0.4712 - val_acc: 0.7744\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.47062\n",
      "Epoch 42/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4462 - acc: 0.7907 - val_loss: 0.4707 - val_acc: 0.7756\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.47062\n",
      "Epoch 43/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4455 - acc: 0.7908 - val_loss: 0.4706 - val_acc: 0.7745\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.47062 to 0.47059, saving model to best.hdf5\n",
      "Epoch 44/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4435 - acc: 0.7915 - val_loss: 0.4696 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.47059 to 0.46956, saving model to best.hdf5\n",
      "Epoch 45/75\n",
      "80250/80250 [==============================] - 1s 12us/step - loss: 0.4435 - acc: 0.7924 - val_loss: 0.4692 - val_acc: 0.7766\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.46956 to 0.46919, saving model to best.hdf5\n",
      "Epoch 46/75\n",
      "80250/80250 [==============================] - 1s 13us/step - loss: 0.4428 - acc: 0.7927 - val_loss: 0.4691 - val_acc: 0.7768\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.46919 to 0.46915, saving model to best.hdf5\n",
      "Epoch 47/75\n",
      "80250/80250 [==============================] - 1s 13us/step - loss: 0.4429 - acc: 0.7942 - val_loss: 0.4690 - val_acc: 0.7764\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.46915 to 0.46900, saving model to best.hdf5\n",
      "Epoch 48/75\n",
      "80250/80250 [==============================] - 1s 13us/step - loss: 0.4431 - acc: 0.7931 - val_loss: 0.4684 - val_acc: 0.7768\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.46900 to 0.46837, saving model to best.hdf5\n",
      "Epoch 49/75\n",
      "80250/80250 [==============================] - 1s 13us/step - loss: 0.4406 - acc: 0.7943 - val_loss: 0.4687 - val_acc: 0.7756\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.46837\n",
      "Epoch 50/75\n",
      "80250/80250 [==============================] - 1s 13us/step - loss: 0.4402 - acc: 0.7945 - val_loss: 0.4678 - val_acc: 0.7769\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.46837 to 0.46780, saving model to best.hdf5\n",
      "Epoch 51/75\n",
      "80250/80250 [==============================] - 1s 13us/step - loss: 0.4394 - acc: 0.7960 - val_loss: 0.4684 - val_acc: 0.7770\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.46780\n",
      "Epoch 52/75\n",
      "80250/80250 [==============================] - 1s 13us/step - loss: 0.4402 - acc: 0.7957 - val_loss: 0.4672 - val_acc: 0.7781\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.46780 to 0.46722, saving model to best.hdf5\n",
      "Epoch 53/75\n",
      "80250/80250 [==============================] - 1s 13us/step - loss: 0.4386 - acc: 0.7961 - val_loss: 0.4672 - val_acc: 0.7776\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.46722 to 0.46719, saving model to best.hdf5\n",
      "Epoch 54/75\n",
      "80250/80250 [==============================] - 1s 12us/step - loss: 0.4378 - acc: 0.7961 - val_loss: 0.4664 - val_acc: 0.7779\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.46719 to 0.46640, saving model to best.hdf5\n",
      "Epoch 55/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4383 - acc: 0.7964 - val_loss: 0.4668 - val_acc: 0.7780\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.46640\n",
      "Epoch 56/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4352 - acc: 0.7971 - val_loss: 0.4665 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.46640\n",
      "Epoch 57/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4348 - acc: 0.7985 - val_loss: 0.4681 - val_acc: 0.7761\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.46640\n",
      "Epoch 58/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4342 - acc: 0.7979 - val_loss: 0.4660 - val_acc: 0.7786\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.46640 to 0.46603, saving model to best.hdf5\n",
      "Epoch 59/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4341 - acc: 0.7986 - val_loss: 0.4670 - val_acc: 0.7775\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.46603\n",
      "Epoch 60/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4335 - acc: 0.7991 - val_loss: 0.4680 - val_acc: 0.7764\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.46603\n",
      "Epoch 61/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4338 - acc: 0.7994 - val_loss: 0.4654 - val_acc: 0.7792\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.46603 to 0.46544, saving model to best.hdf5\n",
      "Epoch 62/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4332 - acc: 0.7997 - val_loss: 0.4656 - val_acc: 0.7791\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.46544\n",
      "Epoch 63/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4328 - acc: 0.7997 - val_loss: 0.4659 - val_acc: 0.7795\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.46544\n",
      "Epoch 64/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4327 - acc: 0.7972 - val_loss: 0.4653 - val_acc: 0.7794\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.46544 to 0.46531, saving model to best.hdf5\n",
      "Epoch 65/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4311 - acc: 0.8011 - val_loss: 0.4648 - val_acc: 0.7804\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.46531 to 0.46484, saving model to best.hdf5\n",
      "Epoch 66/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4305 - acc: 0.8011 - val_loss: 0.4651 - val_acc: 0.7785\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.46484\n",
      "Epoch 67/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4291 - acc: 0.8016 - val_loss: 0.4650 - val_acc: 0.7799\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.46484\n",
      "Epoch 68/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4291 - acc: 0.8027 - val_loss: 0.4656 - val_acc: 0.7787\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.46484\n",
      "Epoch 69/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4284 - acc: 0.8030 - val_loss: 0.4650 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.46484\n",
      "Epoch 70/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4285 - acc: 0.8026 - val_loss: 0.4646 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.46484 to 0.46457, saving model to best.hdf5\n",
      "Epoch 71/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4292 - acc: 0.8013 - val_loss: 0.4644 - val_acc: 0.7811\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.46457 to 0.46439, saving model to best.hdf5\n",
      "Epoch 72/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4278 - acc: 0.8036 - val_loss: 0.4646 - val_acc: 0.7827\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.46439\n",
      "Epoch 73/75\n",
      "80250/80250 [==============================] - 1s 11us/step - loss: 0.4253 - acc: 0.8050 - val_loss: 0.4646 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.46439\n",
      "Epoch 74/75\n",
      "80250/80250 [==============================] - 1s 13us/step - loss: 0.4261 - acc: 0.8042 - val_loss: 0.4648 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.46439\n",
      "Epoch 75/75\n",
      "80250/80250 [==============================] - 1s 13us/step - loss: 0.4256 - acc: 0.8035 - val_loss: 0.4640 - val_acc: 0.7824\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.46439 to 0.46396, saving model to best.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd2f8f35f60>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_aug_sqr, y_train_aug_cat, \n",
    "          batch_size=batch_size, \n",
    "          epochs=75, \n",
    "          validation_data=(X_valid_sqr, y_valid_cat), \n",
    "          callbacks = [checkpoint],\n",
    "          verbose=1) \n",
    "          #class_weight=class_weight_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('last_epoch.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('last_epoch.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80250/80250 [==============================] - 1s 13us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8964086400565867"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_train = model.predict(X_train_aug_sqr, verbose=1)\n",
    "roc_auc_score(y_train_aug_cat[:,0], predictions_train[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 383
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(np.log(predictions_train[:,0]/predictions_train[:,1]), 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8018/8018 [==============================] - 0s 15us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8626164162462574"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_valid_sqr, verbose=1)\n",
    "roc_auc_score(y_valid_cat, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'trained_models/MLP_keras_balanced_{CURRENT_SET}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8018/8018 [==============================] - 0s 14us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8626164162462574"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_valid_sqr, verbose=1)\n",
    "roc_auc_score(y_valid_cat, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 => 0.862447463854684\n",
    "# 1 => 0.8639864740968912\n",
    "# 2 => 0.8584454091674291\n",
    "# 3 => 0.8583561184257356\n",
    "# 4 => 0.8636952850996176\n",
    "# 5 => 0.8657505423877632\n",
    "# 6 => 0.8586787081274665\n",
    "# 7 => 0.8650569108068622\n",
    "# 8 => 0.8596024034021637"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
